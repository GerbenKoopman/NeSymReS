# Unified configuration supporting both wrapper and original functionality
# This configuration extends the original config with wrapper features

# Core paths and data settings (original)
train_path: data/datasets/10000000
val_path: data/validation
raw_test_path: data/raw_datasets/150
test_path: data/validation
model_path: /local/home/lbiggio/NeuralSymbolicRegressionThatScales/weights/10MCompleted.ckpt  

# Basic settings (original + wrapper)
wandb: True
wandb_project: "nesymres"
num_of_workers: 28
batch_size: 25
epochs: 20
val_check_interval: 1.0
precision: 16
gpu: 8

# Wrapper experiment settings
random_seed: 42
output_dir: "experiments"
experiment_name: "default"

# Wrapper training settings
enable_checkpointing: True
save_top_k: 3
early_stopping_patience: 10
early_stopping_min_delta: 0.001
log_every_n_steps: 50
gradient_clip_val: 1.0
accumulate_grad_batches: 1
resume_from_checkpoint: null

# Wrapper logging configuration
logging:
  tensorboard: True
  wandb:
    enabled: ${wandb}
    project: ${wandb_project}
    tags: ["default", "unified"]

# Dataset configurations (original)
dataset_train:
  total_variables: #Do not fill
  total_coefficients: #Do not fill
  max_number_of_points: 800  #2000 before
  type_of_sampling_points: logarithm
  predict_c: True
  fun_support:
    max: 10
    min: -10
  constants:
    num_constants: 3
    additive:
      max: 2
      min: -2
    multiplicative:
      max: 2
      min: -2

dataset_val:
  total_variables: #Do not fill
  total_coefficients: #Do not fill
  max_number_of_points: 500
  type_of_sampling_points: constant
  predict_c: True
  fun_support:
    max: 10
    min: -10
  constants:
    num_constants: 3
    additive:
      max: 2
      min: -2
    multiplicative:
      max: 5
      min: -5

dataset_test:
  total_variables: #Do not fill
  total_coefficients: #Do not fill
  max_number_of_points: 200
  type_of_sampling_points: constant
  predict_c: True
  fun_support:
    max: 10
    min: -10
  constants:
    num_constants: 3
    additive:
      max: 2
      min: -2
    multiplicative:
      max: 2
      min: -2

# Data configuration (wrapper grouping)
data:
  train_path: ${train_path}
  val_path: ${val_path}
  test_path: ${test_path}
  num_workers: ${num_of_workers}
  batch_size: ${batch_size}
  dataset_train: ${dataset_train}
  dataset_val: ${dataset_val}
  dataset_test: ${dataset_test}

# Model architecture (original)
architecture:
  num_of_points: 200
  activation_name: "gelu"
  activation_token: 7
  positional_embedding_num: 20
  
  # SetEncoder parameters (required)
  linear: False
  bit16: True
  norm: True
  mean: 0.5
  std: 0.5
  activation: "relu"
  input_normalization: False
  dim_input: 4
  dim_hidden: 512
  num_heads: 8
  num_inds: 50
  ln: True
  n_l_enc: 5
  num_features: 10
  
  # Model parameters (required)
  sinuisodal_embeddings: False
  trg_pad_idx: 0
  src_pad_idx: 0
  output_dim: 60
  length_eq: 60
  dec_pf_dim: 512
  dec_layers: 5
  dropout: 0.0
  lr: 0.0001
  
  encoder:
    num_encoder_layer: 8
    hidden_size: 512
    num_hidden_layer_encoder: 2
    num_heads: 4
    ffn_hidden_size: 2048
    dropout: 0.
    
  decoder:
    num_decoder_layer: 8
    hidden_size: 512
    num_hidden_layer_decoder: 2
    num_heads: 4
    ffn_hidden_size: 2048
    dropout: 0.

# Inference configuration (original + wrapper)
inference:
  beam_size: 10
  beam_configs:
    - beam_size: 1
      length_penalty: 1.0
      max_len: 50
    - beam_size: 3
      length_penalty: 1.0
      max_len: 75
    - beam_size: 5
      length_penalty: 1.0
      max_len: 100
    - beam_size: 10
      length_penalty: 1.0
      max_len: 150
      
  bfgs:
    activated: True
    n_restarts: 10
    add_coefficients_if_not_existing: True
    normalization_o: 0
    idx_remove: null
    normalization_type: "median"
    stop_time: 100000

# Ablation study configurations (wrapper)
ablation:
  enabled: False
  studies:
    - name: "beam_size_study"
      parameter: "inference.beam_size"
      values: [1, 3, 5, 10, 20]
    - name: "hidden_size_study"
      parameter: "architecture.encoder.hidden_size"
      values: [256, 512, 1024]

# Fine-tuning configurations (wrapper)
finetune:
  enabled: False
  base_model_path: ${model_path}
  learning_rate: 1e-5
  freeze_encoder: False
  freeze_layers: []
  target_datasets: []

# Evaluation configurations (wrapper)
evaluation:
  metrics: ["mse", "r2", "exact_match"]
  save_predictions: True
  save_detailed_results: True
  compare_with_baselines: False
