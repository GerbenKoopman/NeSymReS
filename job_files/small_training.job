#!/bin/bash

#SBATCH --partition=gpu_mig
#SBATCH --gpus=1
#SBATCH --job-name=SmallTraining
#SBATCH --ntasks=1
#SBATCH --cpus-per-task=4
#SBATCH --time=02:00:00
#SBATCH --output=logs/small_training_%A.out

module purge
module load 2024
module load Anaconda3/2024.06-1

source activate nesymres

cd $HOME/nesymres

export PYTHONPATH=$HOME/nesymres/src:$PYTHONPATH

cd scripts

python train.py \
    train_path=../data/raw_datasets/debug/10 \
    val_path=../data/raw_datasets/debug/5 \
    batch_size=4 \
    epochs=5 \
    gpu=1 \
    precision=32 \
    num_of_workers=2 \
    wandb=false \
    architecture.encoder.num_encoder_layer=2 \
    architecture.decoder.num_decoder_layer=2 \
    architecture.encoder.hidden_size=256 \
    architecture.decoder.hidden_size=256 \
    dataset_train.max_number_of_points=200 \
    dataset_val.max_number_of_points=100 \
    log_every_n_steps=10 \
    save_top_k=1 \
    early_stopping_patience=3 \
    hydra.run.dir=../experiments/small_training_$(date +%Y%m%d_%H%M%S)
